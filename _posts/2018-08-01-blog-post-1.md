---
title: 'CS231n Note'
date: 2018-08-01
permalink: /posts/2018/08/cs231n/
tags:
  - computer vision
  - machine learning
  - deep learning
---

Here are the notes for `Convolutional Neural Networks for Visual Recognition`.

Lecture 1: Introduction to Convolutional Neural Networks for Visual Recognition 
------

Increasing computation (CPUs, GPUs) + Relative large high quality dataset (ImageNet) ---> CNN works very well 

Lecture 2: Image Classification
------

We want classifiers that are fast at prediction; slow for training is ok.

$L_1$ distance has coordinate dependency.

Cross-Validation is useful for small datasets, but not used too frequently in deep learning.

Lecture 3: Loss Functions and Optimization
------

Multi-class SVM Classification: Given an example ($x_i$, $y_i$) where $x_i$ is the image and where $y_i$ is the (integer) label, and using the shorthand for the scores vector: $s = f(x_i, W)$.

The SVM loss has the form:

$L_i = \sum_{j \neq y_i}
\begin{cases} 
0,  & \mbox{if }s_{y_i} \ge s_j+1 \\
s_j-s_{y_i}+1, & \mbox{otherwise}
\end{cases}
=\sum_{j \neq y_i}\max(0, s_j-s_{y_i}+1)$

Loss over full dataset is average: 

$L=\frac{1}{N}\sum_{i=1}^NL_i$

In practice: Always use analytic gradient, but check implementation with numerical gradient. This is called a gradient check.


Lecture 4: Introduction to Neural Networks
------

Patterns in BackFlow:

add gate: gradient distributor

max gate: gradient router

mul gate: gradient switcher

Always check: The gradient with respect to a variable should have the same shape as the variable.

Implementations maintain a graph structure, where the nodes implement the `forward()`/`backward()` API.



Lecture 5: Convolutional Neural Networks
------

Low-level features --> Mid-level features --> High-level features --> Linearly separable classifier

Output size: $\frac{N-f}{stride}+1$

Eample time:

Input volume: $32 \times 32 \times 3$

$10$ $5 \times 5$ filters with stride $1$, pad $2$

Number of parameters in this layer? 

Each filter has $5 \times 5 \times 3 + 1 = 76$ parameters --> $76 \times 10 = 760$

Lecture 6: Training Neural Networks I
------

### Activation Funtions:

sigmoid: $\sigma(x) = 1/(1+e^{-x})$

* 3 problems: 1. Saturated neurons "kill" the gradients. 2. Sigmoid outputs are not zero-centered. 3. $\exp()$ is a bit computational expensive.

tanh(x)

* zero centered, still kills gradients when saturated

ReLU (Rectified Linear Unit): $f(x) = \max(0, x)$

* Does not saturate, very compuationally efficient, converges much faster than sigmoid/tanh in practice (e.g. 6x), actually more biologically plausible than sigmoid
* not zero-centered, an annoyance

Leaky ReLU: $f(x) = \max(0.01x, x)$

* will not die

Parametric Rectifier (PReLU): $f(x) = \max(\alpha x, x)$

Exponential Linear Units (ELU): $f(x) =
\begin{cases} 
x,  & \mbox{if }x > 0 \\
\alpha(\exp(x)-1), & \mbox{if } x \le 0
\end{cases}$

* All benefits of ReLU, closer to zero mean outputs, negative saturation regime compared with Leaky ReLU adds some robustness to noise
*  Computation requires $\exp()$

Maxout "Neuron": $\max(w_1^Tx+b_1, w_2^Tx+b_2)$

* Generalizes ReLU and Leaky ReLU, Linear Regime, does not saturate, does not die
* doubles the number of parameters/neuron

### Weight Initialization:

weights too small --> collapse; weights too large --> saturated

Xavier initialization (good for tanh, bad for ReLU)

### Batch Normalization

Consider a batch of activations at some layer. To make each dimension unit gaussian, apply:

\begin{equation}
\bar{x}^{(k)} = \frac{x^{(k)}-E[x^{(k)}]}{\sqrt{Var[x^{(k)}]}}
\end{equation}

1. compute the empirical mean and variance independently for each dimension.
2. normalize.
3. and then allow the network to squash the range if it wants to:

\begin{equation}
y^{(k)} = \gamma^{(k)}\bar{x}^{(k)} + \beta^{(k)}
\end{equation}

Usually inserted after Fully Connected or Convolutional layers, and before nonlinearity.

Note: at test time BatchNorm layer functions differently:

The mean/std are not computed based on the batch. Instead, a single fixed empirical mean of activations during training is used. (e.g. can be estimated during training with running averages)

Random Search > Grid Search

Lecture 7: Training Neural Networks II
------

### Batch Normalization

Input: $x$: $N \times D$

Learnable parameters: $\gamma, \beta$: $D$

Intermediates: $\mu, \sigma$: $D$; $\bar{x}$: $N \times D$

Output: $y$: $N \times D$

$\mu_j = \frac{1}{N}\sum_{i=1}^{N}x_{i,j} \ \ \ \ \ \ \sigma_{j}^2 = \frac{1}{N}\sum_{i=1}^{N}(x_{i,j}-\mu_j)^2$

$\bar{x}_{i,j} = \frac{x_{i,j}-\sigma_j}{\sqrt{\sigma_j^2+\epsilon}} \ \ \ \ \ \ y_{i,j} = \gamma_j\bar{x}_{i,j}+\beta_j$


Lecture 8: Deep Learning Software
------

Lecture 9: CNN Architectures
------

Lecture 10: Recurrent Neural Networks
------

Lecture 11: Detection and Segmentation
------

Lecture 12: Visualizing and Understanding
------

Lecture 13: Generative Models
------

Lecture 14: Deep Reinforcement Learning
------

Lecture 15: Efficient Methods and Hardware for Deep Learning
------

Lecture 16: Adversarial Examples and Adversarial Training
------
