---
title: 'CS231n Note'
date: 2018-08-01
permalink: /posts/2018/08/cs231n/
tags:
  - computer vision
  - machine learning
  - deep learning
---

Here are the notes for `Convolutional Neural Networks for Visual Recognition`.

Lecture 1: Introduction to Convolutional Neural Networks for Visual Recognition 
------

Increasing computation (CPUs, GPUs) + Relative large high quality dataset (ImageNet) ---> CNN works very well 

Lecture 2: Image Classification
------

We want classifiers that are fast at prediction; slow for training is ok.

$L_1$ distance has coordinate dependency.

Cross-Validation is useful for small datasets, but not used too frequently in deep learning.

Lecture 3: Loss Functions and Optimization
------

Multi-class SVM Classification: Given an example ($x_i$, $y_i$) where $x_i$ is the image and where $y_i$ is the (integer) label, and using the shorthand for the scores vector: $s = f(x_i, W)$.

The SVM loss has the form:

$L_i = \sum_{j \neq y_i}
\begin{cases} 
0,  & \mbox{if }s_{y_i} \ge s_j+1 \\
s_j-s_{y_i}+1, & \mbox{otherwise}
\end{cases}
=\sum_{j \neq y_i}\max(0, s_j-s_{y_i}+1)$

Loss over full dataset is average: 

$L=\frac{1}{N}\sum_{i=1}^NL_i$

In practice: Always use analytic gradient, but check implementation with numerical gradient. This is called a gradient check.


Lecture 4: Introduction to Neural Networks
------

Patterns in BackFlow:

add gate: gradient distributor

max gate: gradient router

mul gate: gradient switcher

Always check: The gradient with respect to a variable should have the same shape as the variable.

Implementations maintain a graph structure, where the nodes implement the `forward()`/`backward()` API.



Lecture 5: Convolutional Neural Networks
------

Low-level features --> Mid-level features --> High-level features --> Linearly separable classifier

Output size: $\frac{N-f}{stride}+1$

Eample time:

Input volume: $32 \times 32 \times 3$

$10$ $5 \times 5$ filters with stride $1$, pad $2$

Number of parameters in this layer? 

Each filter has $5 \times 5 \times 3 + 1 = 76$ parameters --> $76 \times 10 = 760$

Lecture 6: Training Neural Networks I
------

### Activation Funtions:

sigmoid: $\sigma(x) = 1/(1+e^{-x})$

* 3 problems: 1. Saturated neurons "kill" the gradients. 2. Sigmoid outputs are not zero-centered. 3. $\exp()$ is a bit computational expensive.

tanh(x)

* zero centered, still kills gradients when saturated

ReLU (Rectified Linear Unit): $f(x) = \max(0, x)$

* Does not saturate, very compuationally efficient, converges much faster than sigmoid/tanh in practice (e.g. 6x), actually more biologically plausible than sigmoid
* not zero-centered, an annoyance

Leaky ReLU: $f(x) = \max(0.01x, x)$

* will not die

Parametric Rectifier (PReLU): $f(x) = \max(\alpha x, x)$

Exponential Linear Units (ELU): $f(x) =
\begin{cases} 
x,  & \mbox{if }x > 0 \\
\alpha(\exp(x)-1), & \mbox{if } x \le 0
\end{cases}$

* All benefits of ReLU, closer to zero mean outputs, negative saturation regime compared with Leaky ReLU adds some robustness to noise
*  Computation requires $\exp()$

Maxout "Neuron": $\max(w_1^Tx+b_1, w_2^Tx+b_2)$

* Generalizes ReLU and Leaky ReLU, Linear Regime, does not saturate, does not die
* doubles the number of parameters/neuron

### Weight Initialization:

weights too small --> collapse; weights too large --> saturated

Xavier initialization (good for tanh, bad for ReLU)

### Batch Normalization

Consider a batch of activations at some layer. To make each dimension unit gaussian, apply:

\begin{equation}
\bar{x}^{(k)} = \frac{x^{(k)}-E[x^{(k)}]}{\sqrt{Var[x^{(k)}]}}
\end{equation}

1. compute the empirical mean and variance independently for each dimension.
2. normalize.
3. and then allow the network to squash the range if it wants to:

\begin{equation}
y^{(k)} = \gamma^{(k)}\bar{x}^{(k)} + \beta^{(k)}
\end{equation}

Usually inserted after Fully Connected or Convolutional layers, and before nonlinearity.

Note: at test time BatchNorm layer functions differently:

The mean/std are not computed based on the batch. Instead, a single fixed empirical mean of activations during training is used. (e.g. can be estimated during training with running averages)

Random Search > Grid Search

Lecture 7: Training Neural Networks II
------

### Batch Normalization

Input: $x$: $N \times D$

Learnable parameters: $\gamma, \beta$: $D$

Intermediates: $\mu, \sigma$: $D$; $\bar{x}$: $N \times D$

Output: $y$: $N \times D$

$\mu_j = \frac{1}{N}\sum_{i=1}^{N}x_{i,j} \ \ \ \ \ \ \sigma_{j}^2 = \frac{1}{N}\sum_{i=1}^{N}(x_{i,j}-\mu_j)^2$

$\bar{x}_{i,j} = \frac{x_{i,j}-\sigma_j}{\sqrt{\sigma_j^2+\epsilon}} \ \ \ \ \ \ y_{i,j} = \gamma_j\bar{x}_{i,j}+\beta_j$

### Fancier Optimization

Loss function has high condition number: ratio of largest to smallest singular value of the Hessian matrix is large.

Saddle point is common in large nueral network.

SGD:

$x_{t+1} = x_{t} - \alpha \nabla f(x_{t})$

SGD + Momentum:

$v_{t+1} = \rho v_{t} + \nabla f(x_t)$

$x_{t+1} = x_{t} - \alpha v_{t+1}$

* Build up "velocity" as a running mean of gradients
* $\rho$ gives "friction"; typically $\rho=0.9$ or $0.99$

SGD + Nesterov Momentum:

$v_{t+1} = \rho v_t - \alpha \nabla f(x_t+\rho v_t)$

$x_{t+1} = x_{t} + v_{t+1}$ 

AdaGrad:

* Added element-wise scaling of the gradient based on the historical sum of squares in each dimension

<pre>
grad_squared = 0
while True:
  dx = compute_gradient(x)
  grad_squared += dx * dx
  x -= learning_rate * dx / (np.sqrt(grad_squared) + 1e-7)
</pre>

RMSProp

<pre>
grad_squared = 0
while True:
  dx = compute_gradient(x)
  grad_squared = decay_rate * grad_squared + (1 - decay_rate) * dx * dx
  x -= learning_rate * dx / (np.sqrt(grad_squared) + 1e-7)
</pre>

Adam (almost)

<pre>
first_moment = 0
second_moment = 0
while True:
  dx = compute_gradient(x)
  first_moment = beta1 * first_moment + (1 - beta1) * dx
  second_moment = beta2 * second_moment + (1 - beta2) * dx * dx # Momentum
  x -= learning_rate * first_moment / (np.sqrt(second_moment) + 1e-7) # AdaGrad/RMSProp
</pre>

At the begining, `second_moment` is too small. The step is too large at the begining.

Adam (Full form)

<pre>
first_moment = 0
second_moment = 0
while True:
  dx = compute_gradient(x)
  first_moment = beta1 * first_moment + (1 - beta1) * dx
  second_moment = beta2 * second_moment + (1 - beta2) * dx * dx # Momentum
  first_unbias = first_moment / (1 - beta1 ** t) # Bias correction
  second_unbias = second_moment / (1 - beta2 ** t) # Bias correction
  x -= learning_rate * first_moment / (np.sqrt(second_moment) + 1e-7) # AdaGrad/RMSProp
</pre>

Bias correction for the fact that first and second moment estimates start at zero.

Adam with $beta1=0.9$, $beta2=0.999$, and $learning_rate=1e-3$ or $5e-4$ is a great starting point for many models.

Learning rate decay over time.

* step decay: e.g. decay learning rate by half every few epochs.

* exponential decay: $\alpha = \alpha_0 e^{-kt}$

* $1/t$ decay: $\alpha = \alpha_0/(1+kt)$

First-Order Optimization

1. Use gradient form linear approxixmation
2. Step to minimize the approximation

Second-Order Optimization

1. Use gradient and Hessian to form quadratic approxximation
2. Step to the minima of the approximation

In practice:

* Adam is a good default choice in most cases

* If you can afford to do full batch updates then try out L-BFGS (and don't forget to disable all sources of noise)

Dropout: remember to rescale in testing time.

Lecture 8: Deep Learning Software
------

Udacity: Intro to Parallel Programming

Static: Once graph is built, can serialize it and run it without the code that built the graph.

Dynamic: Graph building and exxecution are intertwined, so always need to keep code around.

Lecture 9: CNN Architectures
------

LeNet

AleNet

VGG

* Stack of three $3 \times 3$ conv (stride 1) layers has same effective receptive field as one $7 \times 7$ conv layer

* But deeper, more non-linearities

* And fewer parameters: $3 \times (3^2C^2)$ vs. $7^2C^2$ for $C$ channels per layer.

GoogLeNet

* Inception module

* Bottleneck layer to improve efficiency

* 12x less parameters than AlexNet

ResNet

* very deep networks using residual connections

* the problem is an optimization problem, deeper models are harder to optimize

* the deeper model should be able to perform at least as well as the shallower model.

* a solution by construction is copying the learned layers from the shallower model and setting additional layers to identity mapping.

* Stack residual blocks; Every residual block has two $3 \times 3$ conv layers

Network in Network

* Mlpconv layer with "micronetowrk" within each conv layer to compute more abstract features for local patches
 
Lecture 10: Recurrent Neural Networks
------

$h_t = f_W(h_{t-1}, x_t)$

Truncated Backpropagation through time:

* Run forward and backward through chunks of the sequence instead of whole sequence

* Carry hidden states forward in time forever, but only backpropagate for some smaller number of steps

Image Captioning with Attention

Vanilla RNN:

* Largest singular value > 1: eploding gradients (gradient clipping)

* largest singular value < 1: vanishing gradients (advanced architecture)

LSTM:

* two hidden states: hidden state + cell state

GRU

Lecture 11: Detection and Segmentation
------

### Semantic Segmentation

No objects, just pixels. 

Label each pixel in the image with a category label. 

Don't differentiate instances, only care about pixels.

Design network as a bunch of convolutional layers, with downsampling and upsampling inside the network.

Downsampling: pooling, strided convolution

Upsampling: unpooling (nearest neighbor, bed of nails), max unpooling, transpose convolution

### Classification + Localization

Treat localization as a regression problem! (Box Coordinates)

Two loss functions: softmax loss + l2 loss

weighted sum of these two losses

Aside: Human Pose Estimation (Regression loss on several body points)

### Object Detetion

Object dection as Classification: Sliding Window

* Apply a CNN to many different crops of the image, CNN classifies each crop as object or background (Need to apply CNN to huge number of locations and scales)

Region Propsals:

* Find "bloobby" image regions that are likely to contain objects

* R-CNN, Fast R-CNN, Faster R-CNN (region based method)

Detection without Proposals:

* YOLO/SSD

Aside: Object Detection + Captioning = Dense Captioning

### Instance Segmentation

* Mask R-CNN

Lecture 12: Visualizing and Understanding
------

Lecture 13: Generative Models
------

Lecture 14: Deep Reinforcement Learning
------

Lecture 15: Efficient Methods and Hardware for Deep Learning
------

Lecture 16: Adversarial Examples and Adversarial Training
------
