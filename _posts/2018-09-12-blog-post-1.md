---
title: 'Tensorflow for Deep Learning Research'
date: 2018-09-12
permalink: /posts/2018/09/blog-post-1/
tags:
  - TensorFlow
  - Deep Learning
---

[CS 20: Tensorflow for Deep Learning Research](http://web.stanford.edu/class/cs20si/index.html) by Chip Huyen at Stanford University.

## Overview of Tensorflow

To put part of a graph on a specific CPU or GPU:

    # Creates a graph.
    with tf.device('/gpu:2'):
      a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], name='a')
      b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], name='b')
      c = tf.multiply(a, b)

    # Creates a session with log_device_placement set to True.
    sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))

    # Runs the op.
    print(sess.run(c))


tf.Graph()

    g = tf.Graph()

    with g.as_default():
	    x = tf.add(3, 5)
	
    sess = tf.Session(graph=g)
    with tf.Session() as sess:
	    sess.run(x)

Multiple graphs

    g1 = tf.get_default_graph()
    g2 = tf.Graph()

    # add ops to the default graph
    with g1.as_default():
	    a = tf.Constant(3)

    # add ops to the user created graph
    with g2.as_default():
	    b = tf.Constant(5)

## Operations

Visualize with TensorBoard

    import tensorflow as tf

    a = tf.constant(2)
    b = tf.constant(3)
    x = tf.add(a, b)

    writer = tf.summary.FileWriter('./graphs', tf.get_default_graph())
    with tf.Session() as sess:
	    # writer = tf.summary.FileWriter('./graphs', sess.graph) 
	    print(sess.run(x))
    writer.close() # close the writer when youâ€™re done using it
    
Run TensorBoard

    $ python3 [yourprogram].py
    $ tensorboard --logdir="./graphs" --port 6006
    
Variables

    s = tf.get_variable("scalar", initializer=tf.constant(2)) 
    m = tf.get_variable("matrix", initializer=tf.constant([[0, 1], [2, 3]]))
    W = tf.get_variable("big_matrix", shape=(784, 10), initializer=tf.zeros_initializer())
    
Initialize variables

    # The easiest way is initializing all variables at once:
    with tf.Session() as sess:
	    sess.run(tf.global_variables_initializer())

    # Initialize only a subset of variables:
    with tf.Session() as sess:
	    sess.run(tf.variables_initializer([a, b]))

    # Initialize a single variable
    W = tf.Variable(tf.zeros([784,10]))
    with tf.Session() as sess:
	    sess.run(W.initializer)
	    
Each session maintains its own copy of variables

    W = tf.Variable(10)

    sess1 = tf.Session()
    sess2 = tf.Session()

    sess1.run(W.initializer)
    sess2.run(W.initializer)

    print(sess1.run(W.assign_add(10))) 	# >> 20
    print(sess2.run(W.assign_sub(2)))  # >> 8

    print(sess1.run(W.assign_add(100))) # >> 120
    print(sess2.run(W.assign_sub(50)))  # >> -42

    sess1.close()
    sess2.close()

Placeholders

    # create a placeholder for a vector of 3 elements, type tf.float32
    a = tf.placeholder(tf.float32, shape=[3])
    
    b = tf.constant([5, 5, 5], tf.float32)

    # use the placeholder as you would a constant or a variable
    c = a + b  # short for tf.add(a, b)

    with tf.Session() as sess:
	    print(sess.run(c, feed_dict={a: [1, 2, 3]}))

    # >> [6, 7, 8]
    
VERY BAD Lazy loading Example

    x = tf.Variable(10, name='x')
    y = tf.Variable(20, name='y')

    writer = tf.summary.FileWriter('./graphs/normal_loading', tf.get_default_graph())
    with tf.Session() as sess:
	    sess.run(tf.global_variables_initializer())
	    for _ in range(10):
		    sess.run(tf.add(x, y)) # someone decides to be clever to save one line of code
    writer.close()


## Linear and Logistic Regression

Implementing Huber loss

$L_{\delta}(y, f(x)) =
\begin{cases} 
\frac{1}{2}(y-f(x))^2,  & \mbox{for } |y-f(x)|\le \delta, \newline
\delta |y-f(x)| - \frac{1}{2}\delta^2, & \mbox{otherwise}.
\end{cases}$

    def huber_loss(labels, predictions, delta=14.0):
        residual = tf.abs(labels - predictions)
        def f1(): return 0.5 * tf.square(residual)
        def f2(): return delta * residual - 0.5 * tf.square(delta)
        return tf.cond(residual < delta, f1, f2)