---
title: 'Tensorflow for Deep Learning Research'
date: 2018-09-12
permalink: /posts/2018/09/blog-post-1/
tags:
  - TensorFlow
  - Deep Learning
---

[CS 20: Tensorflow for Deep Learning Research](http://web.stanford.edu/class/cs20si/index.html) by Chip Huyen at Stanford University.

## Overview of Tensorflow

To put part of a graph on a specific CPU or GPU:

    # Creates a graph.
    with tf.device('/gpu:2'):
      a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], name='a')
      b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], name='b')
      c = tf.multiply(a, b)

    # Creates a session with log_device_placement set to True.
    sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))

    # Runs the op.
    print(sess.run(c))


tf.Graph()

    g = tf.Graph()

    with g.as_default():
	    x = tf.add(3, 5)
	
    sess = tf.Session(graph=g)
    with tf.Session() as sess:
	    sess.run(x)

Multiple graphs

    g1 = tf.get_default_graph()
    g2 = tf.Graph()

    # add ops to the default graph
    with g1.as_default():
	    a = tf.Constant(3)

    # add ops to the user created graph
    with g2.as_default():
	    b = tf.Constant(5)

## Operations

Visualize with TensorBoard

    import tensorflow as tf

    a = tf.constant(2)
    b = tf.constant(3)
    x = tf.add(a, b)

    writer = tf.summary.FileWriter('./graphs', tf.get_default_graph())
    with tf.Session() as sess:
	    # writer = tf.summary.FileWriter('./graphs', sess.graph) 
	    print(sess.run(x))
    writer.close() # close the writer when youâ€™re done using it
    
Run TensorBoard

    $ python3 [yourprogram].py
    $ tensorboard --logdir="./graphs" --port 6006
    
Variables

    s = tf.get_variable("scalar", initializer=tf.constant(2)) 
    m = tf.get_variable("matrix", initializer=tf.constant([[0, 1], [2, 3]]))
    W = tf.get_variable("big_matrix", shape=(784, 10), initializer=tf.zeros_initializer())
    
Initialize variables

    # The easiest way is initializing all variables at once:
    with tf.Session() as sess:
	    sess.run(tf.global_variables_initializer())

    # Initialize only a subset of variables:
    with tf.Session() as sess:
	    sess.run(tf.variables_initializer([a, b]))

    # Initialize a single variable
    W = tf.Variable(tf.zeros([784,10]))
    with tf.Session() as sess:
	    sess.run(W.initializer)
	    
Each session maintains its own copy of variables

    W = tf.Variable(10)

    sess1 = tf.Session()
    sess2 = tf.Session()

    sess1.run(W.initializer)
    sess2.run(W.initializer)

    print(sess1.run(W.assign_add(10))) 	# >> 20
    print(sess2.run(W.assign_sub(2)))  # >> 8

    print(sess1.run(W.assign_add(100))) # >> 120
    print(sess2.run(W.assign_sub(50)))  # >> -42

    sess1.close()
    sess2.close()

Placeholders

    # create a placeholder for a vector of 3 elements, type tf.float32
    a = tf.placeholder(tf.float32, shape=[3])
    
    b = tf.constant([5, 5, 5], tf.float32)

    # use the placeholder as you would a constant or a variable
    c = a + b  # short for tf.add(a, b)

    with tf.Session() as sess:
	    print(sess.run(c, feed_dict={a: [1, 2, 3]}))

    # >> [6, 7, 8]
    
VERY BAD Lazy loading Example

    x = tf.Variable(10, name='x')
    y = tf.Variable(20, name='y')

    writer = tf.summary.FileWriter('./graphs/normal_loading', tf.get_default_graph())
    with tf.Session() as sess:
	    sess.run(tf.global_variables_initializer())
	    for _ in range(10):
		    sess.run(tf.add(x, y)) # someone decides to be clever to save one line of code
    writer.close()


## Linear and Logistic Regression

Implementing Huber loss

$L_{\delta}(y, f(x)) =
\begin{cases} 
\frac{1}{2}(y-f(x))^2,  & \mbox{for } |y-f(x)|\le \delta, \newline
\delta |y-f(x)| - \frac{1}{2}\delta^2, & \mbox{otherwise}.
\end{cases}$

    def huber_loss(labels, predictions, delta=14.0):
        residual = tf.abs(labels - predictions)
        def f1(): return 0.5 * tf.square(residual)
        def f2(): return delta * residual - 0.5 * tf.square(delta)
        return tf.cond(residual < delta, f1, f2)
        
tf.data.Dataset

    tf.data.Dataset.from_tensor_slices((features, labels))
    dataset = tf.data.Dataset.from_tensor_slices((data[:,0], data[:,1]))
    
    print(dataset.output_types)    # >> (tf.float32, tf.float32)
    print(dataset.output_shapes)   # >> (TensorShape([]), TensorShape([]))
    
Create Dataset from files

    tf.data.TextLineDataset(filenames)
    tf.data.FixedLengthRecordDataset(filenames)
    tf.data.TFRecordDataset(filenames)
    
tf.data.Iterator

    iterator = dataset.make_one_shot_iterator()
    X, Y = iterator.get_next()         # X is the birth rate, Y is the life expectancy
    
    with tf.Session() as sess:
	    print(sess.run([X, Y]))		# >> [1.822, 74.82825]
	    print(sess.run([X, Y]))		# >> [3.869, 70.81949]
	    print(sess.run([X, Y]))		# >> [3.911, 72.15066]
	    
	 
	    
    iterator = dataset.make_initializable_iterator()
    ...
    for i in range(100): 
        sess.run(iterator.initializer) 
        total_loss = 0
        try:
            while True:
                sess.run([optimizer]) 
        except tf.errors.OutOfRangeError:
            pass
            

## Eager execution

Boilerplate

    x = tf.placeholder(tf.float32, shape=[1, 1])
    m = tf.matmul(x, x)

    print(m)
    # Tensor("MatMul:0", shape=(1, 1), dtype=float32)

    with tf.Session() as sess:
      m_out = sess.run(m, feed_dict={x: [[2.]]})
    print(m_out)
    # [[4.]]
    
    ##########
    
    x = [[2.]]  # No need for placeholders!
    m = tf.matmul(x, x)

    print(m)  # No sessions!
    # tf.Tensor([[4.]], shape=(1, 1), dtype=float32)
    
Lazy Loading

    x = tf.random_uniform([2, 2])

    with tf.Session() as sess:
      for i in range(x.shape[0]):
        for j in range(x.shape[1]):
          print(sess.run(x[i, j]))
          
    ##########
    
    x = tf.random_uniform([2, 2])

    for i in range(x.shape[0]):
      for j in range(x.shape[1]):
        print(x[i, j])
        
Tensors Act Like NumPy Arrays

    x = tf.constant([1.0, 2.0, 3.0])

    # Tensors are backed by NumPy arrays
    assert type(x.numpy()) == np.ndarray
    squared = np.square(x) # Tensors are compatible with NumPy functions
 
    # Tensors are iterable!
    for i in x:
      print(i)
      
Gradients

    def square(x):
      return x ** 2

    grad = tfe.gradients_function(square)

    print(square(3.))    # tf.Tensor(9., shape=(), dtype=float32)
    print(grad(3.))      # [tf.Tensor(6., shape=(), dtype=float32)]
    
    
    x = tfe.Variable(2.0)
    def loss(y):
      return (y - x ** 2) ** 2

    grad = tfe.implicit_gradients(loss)

    print(loss(7.))  # tf.Tensor(9., shape=(), dtype=float32)
    print(grad(7.))  # [(<tf.Tensor: -24.0, shape=(), dtype=float32>, <tf.Variable 'Variable:0', shape=() dtype=float32, numpy=2.0>)]
    
  
## Variable sharing and managing experiments

Word Embedding

1. CBOW: use neighbors to predict center
2. Skip-Gram: use center to predict neighbors

Structure TensorFlow model

    class SkipGramModel:
        """ Build the graph for word2vec model """
        def __init__(self, params):
            pass

        def _import_data(self):
            """ Step 1: import data """
            pass

        def _create_embedding(self):
            """ Step 2: define weights. In word2vec, it's actually the weights that we care about """
            pass

        def _create_loss(self):
            """ Step 3 + 4: define the inference + the loss function """
            pass

        def _create_optimizer(self):
            """ Step 5: define optimizer """
            pass
